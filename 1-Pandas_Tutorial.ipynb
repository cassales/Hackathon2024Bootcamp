{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cassales/Hackathon2024Bootcamp/blob/master/Pandas_TS_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pandas tutorial"
      ],
      "metadata": {
        "id": "SVPAbg2VilR7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNVmwyiv8VFe"
      },
      "source": [
        "### In the following block we are going to import some standard libraries used for reading data and visualizing them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wds8x0jJtoA1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2Iw1dTle7KT"
      },
      "source": [
        "### Here we are going to download the csv file we have prepared, this file contains the levels of the river stage sensors and the rain gauges in the Coromandel region. We can run bash commands using by prepending the line with a '!'.\n",
        "\n",
        "You can get a copy of the file here @ https://drive.google.com/file/d/1IsIl5465JevH2WJVasEhWDVxk_40vm8L/view?usp=sharing and upload it manually"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/nlim-uow/my_notes/main/river_rain.csv"
      ],
      "metadata": {
        "id": "8_5SPHfBsEYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-FU395MyS4x"
      },
      "source": [
        "### In the next block we are going to get the contents of the csv file and store that as our variable \"myDataFrame\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNT4AewXwLff"
      },
      "outputs": [],
      "source": [
        "myDataFrame=pd.read_csv('river_rain.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBgD6w9nyj6R"
      },
      "source": [
        "#### Here we are going to look into the headers and obtain some information regarding myDataFrame, there are a few things we should note here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnsDxiFFwk25"
      },
      "outputs": [],
      "source": [
        "myDataFrame.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6z1LLZ13j-6"
      },
      "source": [
        "a) Note the number of entries after the Index. Here it shows that we have 259449 entries, you will note that, the column Pinnacles Rain contains missing entries and has less than 259449 entries.\n",
        "\n",
        "b) the other thing to note here is the Data Type of each of the columns, here the columns dd/MM/yyyy and hh:mm:ss is read in as \"object\" (that is to say they are read in as strings). So Pandas has no notion that they contain the date and time of the entries. This is something we will resolve later, when we get to the advanced section of this tutorial\n",
        "\n",
        "c) The other four columns should be numeric (float64). You will notice that Opitonui is read in as an \"object\" (text). This is usually caused by one of the entries in the column containing text instead of numerical values. (in this case we intentionally coded a \"not available\" value as \"N.A.\" which causes pandas to think that this column contains texts)\n",
        "\n",
        "d) Note here that the info didn't show any issues with \"CR Rain\", however the missing values in \"CR Rain\" is coded as -99. While info() is a very helpful tool, it may not reveal all the problem with the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpF13EOO19v4"
      },
      "source": [
        "### Resolving missing entries\n",
        "In the next block we are going to code the all text entries in the column Opitonui as missing values. This calls an in-built function in pandas that retains all numeric values in the column and convert all non-numeric values into \"not available\" (i.e. \"missing value\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7bMw2Ic18lN"
      },
      "outputs": [],
      "source": [
        "myDataFrame['CR Rain'] = pd.to_numeric(myDataFrame['CR Rain'], errors='coerce')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxPElCdl5nSY"
      },
      "source": [
        "It is usually a good practice to look at the info again after converting, to ensure that:\n",
        "\n",
        "a) the conversion is done correctly\n",
        "\n",
        "b) not too many entries are dropped in the conversion process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_Tg_wak5lq2"
      },
      "outputs": [],
      "source": [
        "myDataFrame.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqoWj0np6RfF"
      },
      "source": [
        "Another good practice to do is to quickly visualize the data and obtain some summary statistics, we will go deeper into this later. This will reveal that the person supplying the data used -99 to code \"missing\" in the Opitonui column.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8UWvJ3e6QQx"
      },
      "outputs": [],
      "source": [
        "myDataFrame.plot.box()\n",
        "plt.show()\n",
        "myDataFrame.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjBf_e8E61Sp"
      },
      "source": [
        "Notice that the boxplot and the min value for \"Opitonui\" is contains an impossible value '-99'. We will now resolve this by replacing all occurance of -99 with \"missing\" (note here that np.nan is a special variable for not a number/missing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WO2YiUj60cK"
      },
      "outputs": [],
      "source": [
        "myDataFrame['Opitonui']=myDataFrame['Opitonui'].replace(-99,np.nan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHCq-Ng_8_Js"
      },
      "source": [
        "Again, it is a good practice to visualize the data and obtain some summary statistics before proceeding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fu6T_l1M895u"
      },
      "outputs": [],
      "source": [
        "myDataFrame.plot.box()\n",
        "plt.show()\n",
        "myDataFrame.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecLA3GVx9MB6"
      },
      "source": [
        "Now we are ready to deal with the missing values. There are some common methods to handle missing values\n",
        "\n",
        "a) drop the entries with missing values\n",
        "\n",
        "b) fill the missing values with the mean/median of the columns\n",
        "\n",
        "c) fill the missing values with the preceding values\n",
        "\n",
        "d) apply interpolation to fill in the missing values (will be covered in Advanced Pandas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7a_TruI_Fbz"
      },
      "source": [
        "Droping entries with missing values is straight forward, dropna() to create a new dataframe that has the missing values dropped. Notice that the new dataframe is smaller and only contains the entries where the rows are complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oSssds29KvS"
      },
      "outputs": [],
      "source": [
        "myDataFrame_dropping_na=myDataFrame.dropna()\n",
        "myDataFrame_dropping_na.info()\n",
        "myDataFrame_dropping_na.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M-Adj3Z_16i"
      },
      "source": [
        "We can use the function fillna() to fill in the values for the missing entries with another value, in the first block we are replacing the missing value with the median of the column, and in the second block we are replacing the missing value with the mean of the column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtyhBcmm-Qv9"
      },
      "outputs": [],
      "source": [
        "# fill missing values with median for numeric columns only\n",
        "numeric_columns = myDataFrame.select_dtypes(include=['number']).columns\n",
        "myDataFrame_fill_median = myDataFrame.fillna(myDataFrame[numeric_columns].median())\n",
        "myDataFrame_fill_median.info()\n",
        "myDataFrame_fill_median.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77H3OoK48Wtq"
      },
      "outputs": [],
      "source": [
        "myDataFrame_fill_median.values.shape\n",
        "myDataFrame_fill_median['Tairua']+(np.random.randn(len(myDataFrame_fill_median))*myDataFrame_fill_median['Tairua'].std()*0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzDmZnHO_p9n"
      },
      "outputs": [],
      "source": [
        "myDataFrame_fill_mean=myDataFrame.fillna(myDataFrame[numeric_columns].mean())\n",
        "myDataFrame_fill_mean.info()\n",
        "myDataFrame_fill_mean.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKmb3YU-ALaz"
      },
      "source": [
        "We can also use the argument 'method=\"ffill\"' to fill the missing value with the preceding values. Note that if the first entry contains missing values, the resulting dataframe will still contain missing value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h12RPTYoAKvI"
      },
      "outputs": [],
      "source": [
        "myDataFrame_fill_forward=myDataFrame.fillna(method='ffill')\n",
        "myDataFrame_fill_forward.info()\n",
        "myDataFrame_fill_forward.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdQMQKuTCbXA"
      },
      "source": [
        "For the fourth method, we will come back to that later when we get into the more advanced topic, as it would require providing pandas/python with a bit more information before we are able to do that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS8puxF7DTDs"
      },
      "source": [
        "### Data visualization and summarization\n",
        "Pandas has some in-built visualization and we can use either Seaborn or any of the other plotting libraries to supplement the inbuilt visualization. If you have multiple plot commands in the same plot and you want a new figure for each plot, it is usually good practice to end every figure with plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6AzQlWXFfhe"
      },
      "source": [
        "Plotting a kernel density plot (histogram) of the data. You can either combine them within the same figure, or have each column individually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKF02boDDzU-"
      },
      "outputs": [],
      "source": [
        "myDataFrame.plot.kde()\n",
        "plt.show()\n",
        "myDataFrame['CR Rain'].plot.kde()\n",
        "plt.show()\n",
        "myDataFrame['Pinnacles Rain'].plot.kde()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCwvbBgfGwIy"
      },
      "source": [
        "If you prefer bins, we can plot this alternate histogram of the data. You can either combine them within the same figure, or have each column individually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vYTGvniD_Z3"
      },
      "outputs": [],
      "source": [
        "myDataFrame.plot.hist(bins=100)\n",
        "plt.show()\n",
        "myDataFrame['Tairua'].plot.hist(bins=30)\n",
        "plt.show()\n",
        "myDataFrame['Opitonui'].plot.hist(bins=30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIX9smCkdUSn"
      },
      "source": [
        "We can also visualize the data sequentially as a line plot\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BK4opunxdb5U"
      },
      "outputs": [],
      "source": [
        "myDataFrame.plot.line()\n",
        "plt.show()\n",
        "myDataFrame['Tairua'].plot.line()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szG3PTg7dnKX"
      },
      "source": [
        "Or visualize the relationship between two columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur6KBXdcdmUX"
      },
      "outputs": [],
      "source": [
        "myDataFrame.plot.scatter('Pinnacles Rain','CR Rain')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_ce-PAyd686"
      },
      "source": [
        "The in-built pandas methods are quite powerful, however it may be too limited for some users, here we will look at another library called seaborn that has more plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cuPKclngSvf"
      },
      "source": [
        "Here we see a pairwise scatter plot using the library seaborn, you can look at the documentations for additional control as to what goes to the different axes, or to add additional annotations on to the plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40wTlPO_djnb"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(data=myDataFrame)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZjOzaqLhDUf"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(data=myDataFrame, x='Tairua',y='Opitonui', size='CR Rain')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqoHd6CCgg34"
      },
      "outputs": [],
      "source": [
        "sns.violinplot(x=myDataFrame['Tairua'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3ycwKk2iPgc"
      },
      "source": [
        "Other python plotting libraries are altair/plotly/bokeh, however these libraries do not work well with colab, and may crash if you are dealing with large datasets, I will show an example with the aggregated / filtered data in a later example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz6QSKwTiOis"
      },
      "outputs": [],
      "source": [
        "#import altair as alt\n",
        "#alt.Chart(myDataFrame).mark_point().encode(x='Pinnacles Rain',y='CR Rain')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3HsryfJA-Z4"
      },
      "source": [
        "### Data accessing and manipulations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moJweBj7BXAQ"
      },
      "source": [
        "We can concatenate text (objects) using the + operator, here we are showing what happens when we concatenate the first two columns together and joining them with a space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqnv4WdeA9ws"
      },
      "outputs": [],
      "source": [
        "display(myDataFrame['dd/MM/yyyy']+\" \"+myDataFrame['hh:mm:ss'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLWgltrrBvUq"
      },
      "source": [
        "We can also do any numerical operation we want to the numerical columns. Note here that if the operation is mathematically invalid (i/e divide by 0), it would return NaN and would be treated as \"missing\". Also note that any operation with a missing value is automatically treated as invalid, which is why it may make sense to do the operation after you have resolved the missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrRT_jQD0sSq"
      },
      "outputs": [],
      "source": [
        "print('Tairua + Pinnacles')\n",
        "display(myDataFrame_fill_median['Tairua']+myDataFrame_fill_median['Pinnacles Rain'])\n",
        "print('Tairua/Opitonui')\n",
        "display(myDataFrame_fill_median['Tairua']/myDataFrame_fill_median['Opitonui'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLpgDtg1sZDN"
      },
      "source": [
        "#### Creating new columns and assigning values based on other columns\n",
        "Creating new columns programmatically is very straight forward in Pandas/Python. All we have to do is to assign the outputs to the dataframe and the corresponding column name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pz-9Azzorvio"
      },
      "outputs": [],
      "source": [
        "myDataFrame_fill_median['River Ratio']=myDataFrame_fill_median['Tairua']/myDataFrame_fill_median['Opitonui']\n",
        "myDataFrame_fill_median['Total Rain']=myDataFrame_fill_median['CR Rain']+myDataFrame_fill_median['Pinnacles Rain']\n",
        "display(myDataFrame_fill_median.describe())\n",
        "\n",
        "myDataFrame_fill_median['River Ratio'].plot.kde()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRGTnrsYloD_"
      },
      "source": [
        "We have already looked at methods to obtain the statistics, note that we are not limited to just the standard summary statistics, pandas support more statistics than that"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bm8Ge8oF3v-s"
      },
      "outputs": [],
      "source": [
        "print(\"The average for each column:\")\n",
        "display(myDataFrame_fill_median.mean(numeric_only=True))\n",
        "print(\"\")\n",
        "print(\"The variance for each column:\")\n",
        "display(myDataFrame_fill_median.var(numeric_only=True))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Table of the summary statistics in each column:\")\n",
        "display(myDataFrame_fill_median.describe())\n",
        "\n",
        "print(\"\")\n",
        "print(\"These are some of the additional statistics supported:\")\n",
        "display(myDataFrame_fill_median[numeric_columns].agg(['sum','min','max','mean','median','var','std','kurt','skew','nunique']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDQU24rsmVZP"
      },
      "source": [
        "We can address contents in the data frame by location and by value. For instance, if we want to find out what is the river level of Tairua at the start of the dataset, we can access it by using the following\n",
        "`myDataFrame.loc[0,'Tairua']`.\n",
        "\n",
        "Note that in python we use zero based index meaning the first entry starts with 0. You can also access the values using the index of the column myDataFrame.iloc[0,2]. Personally I prefer iloc, as it is more consistent and functions better when the columns are not properlly named"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sOlsuxGn3xm"
      },
      "outputs": [],
      "source": [
        "myDataFrame_fill_median.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUkBXteTmTVz"
      },
      "outputs": [],
      "source": [
        "print(myDataFrame_fill_median.loc[0,'Tairua'])\n",
        "print(myDataFrame_fill_median.iloc[0,2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQP-spDGn9vD"
      },
      "source": [
        "You can also display a range of values. The following commands display, respectively: the first 5 entries, the last 5 entries, and the 100th to 110th entries for the Tairua river. Notice that the index is **not**\n",
        " \"right\"-inclusive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6B91mdEznooA"
      },
      "outputs": [],
      "source": [
        "print('1 - First 5 entries\\n', myDataFrame_fill_median.iloc[0:5,2])\n",
        "print('\\n2  - Last 5 entries\\n', myDataFrame_fill_median.iloc[-5:,2])\n",
        "print('\\n3 - 100th to 110th entries\\n', myDataFrame_fill_median.iloc[100:110,2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRqJ96oC1mGv"
      },
      "source": [
        "Obtaining the values for all columns in entries 100-109"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQEhoNFf0dZw"
      },
      "outputs": [],
      "source": [
        "display(myDataFrame_fill_median.iloc[100:110,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g333KiZL18-T"
      },
      "source": [
        "Obtaining values in a column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFZEuTfY16ey"
      },
      "outputs": [],
      "source": [
        "print(myDataFrame_fill_median['Tairua'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg6mBc2R2bea"
      },
      "outputs": [],
      "source": [
        "display(myDataFrame_fill_median[['Tairua','Opitonui']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULHCeehoox01"
      },
      "source": [
        "#### Accessing / subsetting dataframes by values.\n",
        "We can also access specific rows of the dataframe by conditions in the dataframe. In the following example we are filtering the dataset to just when the \"CR rain\" is greater than 0, storing that as myDataFrameFiltered. and we will then visualize the river levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utKaoe2ipj0C"
      },
      "outputs": [],
      "source": [
        "myDataFrame_filtered=myDataFrame_fill_median[myDataFrame_fill_median['CR Rain']>0]\n",
        "display(myDataFrame_filtered.describe())\n",
        "myDataFrame_filtered['Tairua'].plot.hist(bins=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAHXGLH2rPca"
      },
      "source": [
        "## Advanced pandas methods\n",
        "### Converting columns to datetime type\n",
        "The following command tells pandas how to parse the text and understand it as a date time. Here we are concatenating the date column and the time column, and then converting it to a datetime this gives pandas a notion of time in the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sabYR_ck0TcX"
      },
      "outputs": [],
      "source": [
        "myDataFrame['datetime_unparsed']=myDataFrame['dd/MM/yyyy']+\" \"+myDataFrame['hh:mm:ss']\n",
        "myDataFrame['datetime_parsed']=pd.to_datetime(myDataFrame['datetime_unparsed'],format='%d/%m/%Y %H:%M:%S')\n",
        "myDataFrame.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEAo_H36vR9r"
      },
      "source": [
        "It is usually helpful to set the datetime format eventhough Pandas can usually infer the format, mainly because Pandas tend to use the US format of month-day-year if there are not enough non US dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pVOctaB1p0K"
      },
      "outputs": [],
      "source": [
        "myDataFrame_datetime=myDataFrame.set_index('datetime_parsed')\n",
        "display(myDataFrame_datetime)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bgIs7Upvy-_"
      },
      "source": [
        "(Optional) Let's drop the redundant columns as it helps with processing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uouKfOQRwHTU"
      },
      "outputs": [],
      "source": [
        "myDataFrame_datetime=myDataFrame_datetime.drop(columns=['dd/MM/yyyy','hh:mm:ss','datetime_unparsed'])\n",
        "myDataFrame_datetime.info()\n",
        "myDataFrame_datetime.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkyF-xquwuoT"
      },
      "source": [
        "Now that pandas \"understand\" the data as a time series, we are able to do more advanced features with the data. As I've alluded earlier, let's try filling the NA values by interpolating the values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZSOzTDBwsvN"
      },
      "outputs": [],
      "source": [
        "myDataFrame_datetime_fill_interpolate=myDataFrame_datetime.interpolate(method='time')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JziZiGiXzh8A"
      },
      "source": [
        "### Creating rolling sums\n",
        "One of the useful features in pandas to create a rolling sum, i.e. to get the total rain over a period of time. Note that you may use any aggregation function not neccessarily the sum().\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWyAh7cxzg74"
      },
      "outputs": [],
      "source": [
        "myDataFrame_datetime_rolling_sum=myDataFrame_datetime_fill_interpolate\n",
        "myDataFrame_datetime_rolling_sum['12 Hour CR Rain']=myDataFrame_datetime_fill_interpolate['CR Rain'].rolling('12H').sum()\n",
        "myDataFrame_datetime_rolling_sum['24 Hour CR Rain']=myDataFrame_datetime_fill_interpolate['CR Rain'].rolling('24H').sum()\n",
        "myDataFrame_datetime_rolling_sum['48 Hour CR Rain']=myDataFrame_datetime_fill_interpolate['CR Rain'].rolling('2D').sum()\n",
        "myDataFrame_datetime_rolling_sum['96 Hour CR Rain']=myDataFrame_datetime_fill_interpolate['CR Rain'].rolling('4D').sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D83ffMyG0SoR"
      },
      "outputs": [],
      "source": [
        "sns.kdeplot(data=myDataFrame_datetime_rolling_sum, x='96 Hour CR Rain', fill=True)\n",
        "plt.show()\n",
        "display(myDataFrame_datetime_rolling_sum.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxmq6hn46hmc"
      },
      "source": [
        "Another thing we can do is to resample the data at a different frequency, in the example below we are resampling the data at a different sample rate. When we resample at a you may choose to use a single aggregating function, or choose to aggregate it differently by column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFYPbZSu5g8N"
      },
      "outputs": [],
      "source": [
        "myDataFrame_1h_resampled=myDataFrame_datetime_fill_interpolate.resample('1H').agg({'Tairua': 'mean',\n",
        "                             'Opitonui':'mean',\n",
        "                             'CR Rain':'sum',\n",
        "                             'Pinnacles Rain': 'sum'})\n",
        "display(myDataFrame_1h_resampled)\n",
        "myDataFrame_24h_resampled=myDataFrame_datetime_fill_interpolate.resample('24H').agg({'Tairua': 'mean',\n",
        "                             'Opitonui':'mean',\n",
        "                             'CR Rain':'sum',\n",
        "                             'Pinnacles Rain': 'sum'})\n",
        "display(myDataFrame_24h_resampled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNyWCVgc7eE2"
      },
      "source": [
        "You may resample it at a higher frequency, as well, however notice what happens when you choose the same aggregating function as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuw7C8_J7VwL"
      },
      "outputs": [],
      "source": [
        "myDataFrame_2p5m_resampled=myDataFrame_datetime_fill_interpolate.resample('2.5T').agg({'Tairua': 'mean',\n",
        "                             'Opitonui':'mean',\n",
        "                             'CR Rain':'sum',\n",
        "                             'Pinnacles Rain': 'sum'})\n",
        "display(myDataFrame_2p5m_resampled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHpQWoe17qoH"
      },
      "source": [
        "In this case, it would make more sense to instead combine what we done previously and interpolate to obtain the inbetween values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfS79_Yu8ET1"
      },
      "outputs": [],
      "source": [
        "myDataFrame_2p5m_resampled=myDataFrame_datetime_fill_interpolate.resample('2.5T').interpolate(method='time')\n",
        "display(myDataFrame_2p5m_resampled)\n",
        "myDataFrame_2p5m_resampled.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2rmiaP8yyMR"
      },
      "source": [
        "One very useful thing we can also do that may not appear to make sense at first is to resample at the same sampling rate as the original data. This is useful if you suspect that your data is missing entries. (Here we are missing about 33 entries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E1oujKnzIwW"
      },
      "outputs": [],
      "source": [
        "myDataFrame_5m_resampled=myDataFrame_datetime_fill_interpolate.resample('5T').interpolate(method='time')\n",
        "display(myDataFrame_5m_resampled)\n",
        "display(myDataFrame_5m_resampled.info())\n",
        "display(myDataFrame_datetime_fill_interpolate.info())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx1u56Yi8h2Y"
      },
      "source": [
        "### Group by, sub tables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87LmR3AE8puw"
      },
      "source": [
        "In the next block I am going to create some new columns, here I am extracting the month, and hour from the index and store that as the a new column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogkE9j8qz6t2"
      },
      "outputs": [],
      "source": [
        "myDataFrame_1h_resampled['month']=myDataFrame_1h_resampled.index.month\n",
        "myDataFrame_1h_resampled['hour']=myDataFrame_1h_resampled.index.hour\n",
        "myDataFrame_1h_resampled.info()\n",
        "myDataFrame_1h_resampled.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jt_cqsjvMSBw"
      },
      "outputs": [],
      "source": [
        "display(myDataFrame_1h_resampled.groupby('month').mean())\n",
        "display(myDataFrame_1h_resampled.groupby('hour').mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qybXbLSs9sGT"
      },
      "outputs": [],
      "source": [
        "display(myDataFrame_1h_resampled.groupby('month').agg(['mean','std','min','max']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "770Xez1UNAcB"
      },
      "outputs": [],
      "source": [
        "import altair as alt\n",
        "alt.Chart(myDataFrame_1h_resampled[myDataFrame_1h_resampled['month']==3]).mark_point().encode(x='CR Rain',y='Tairua').interactive()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byInN4DZFPpv"
      },
      "source": [
        "We can also shift the values in the data, in this example we want to create a new column called future Tairua, which shows what would the river stage be 1 hour in the future, this will come in handy when we want to use python to do machine learning to predict the river stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqVQ7bIqCp3j"
      },
      "outputs": [],
      "source": [
        "myDataFrame_1h_resampled['future Tairua']=myDataFrame_1h_resampled['Tairua'].shift(-1, freq='H')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0UB9sgXFwM-"
      },
      "source": [
        "The next plot shows us a scatter plot of the current river stage and the future river stage. Here we show that the two values are fairly correlated and it may be possible to obtain good results using simple machine learning methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ehbwGrlDBi5"
      },
      "outputs": [],
      "source": [
        "myDataFrame_1h_resampled.plot.scatter('Tairua','future Tairua')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8i5Eg57D1AIi"
      },
      "source": [
        "#### Combining data frames\n",
        "We can use the join command to combine two dataframes, the keyword rsuffix tells pandas to add \"_new\" to any column. The join command automatically tries to get the entries that matches the index and append that to the output dataframe. Notice that not all the records from the rolling sum dataframe could be appended to the"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtgekTZZz3-3"
      },
      "outputs": [],
      "source": [
        "myDataFrame_1h_resampled_joined=myDataFrame_1h_resampled.join(myDataFrame_datetime_rolling_sum,rsuffix='_new')\n",
        "myDataFrame_1h_resampled.info()\n",
        "myDataFrame_datetime_rolling_sum.info()\n",
        "myDataFrame_1h_resampled_joined.info()\n",
        "display(myDataFrame_1h_resampled_joined)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}